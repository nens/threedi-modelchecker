- Start Date: 2021-02-09
- RFC PR: https://github.com/nens/threedi-modelchecker/35

# Summary

Add an API for reading from and writing to 3Di model schematisation (sqlite)
files.

# Basic examples

Initialisation:

```
>>> from threedi_modelchecker import ThreediDatabase
>>> db = ThreediDatabase("/path/to/sqlite")
```

Reading records: SQLAlchemy API
```
>>> from threedi_modelchecker import models
>>> with db.session_scope() as session:
...     query = session.query(models.Channel)
...     channel = query.first()
...     all_channels = query.all()
...     specific_channel = channels.filter_by(display_name="577").one()
```

Writing records: SQLAlchemy API
```
>>> with db.session_scope() as session:
...     session.add(models.Channel(...))
...     session.delete(schematisation.channels.first())
```

Bulk input to numpy
```
>>> with db.session_scope() as session:
...     query = session.query(models.Channel)
...     query.bulk_read_all(fields=["the_geom", "dist_calc_points"])
{
    "the_geom": array([pygeos.Geometry LINESTRING ..., ...]),
    "dist_calc_points": array([100., ...]),
}
```

# Motivation

3Di has accumulated several implementations for the schematisation I/O.

- Django model definition in the old stack (https://github.com/nens/threedi/tree/master/lib/threedi-tools/threedi_tools).
  This is used to create a so-called `v2_workdbs`, and includes 172 migrations
  implemented in a very old django version (1.6) that still used south migrations.
- An ORM in the old stack that reads data from the sqlite during the input file generation. 
  https://github.com/nens/threedi/tree/master/lib/threedi-spatialite-tools/threedi_spatialite_tools.
- threedi-modelchecker: SQLAlchemy implementation used to validate schematisations both server-side and client-side.
- model-importer: Explorative implementation in SQLAlchemy: https://github.com/nens/model-importer/tree/future-orm

We need a 'single source of truth' for the model definition. From the four
above implementations the third option (threedi-modelchecker)
appears to be the best to continue expanding into an ORM. It is using current
techniques and is in a production-ready state.

Additional requirements (outside of this RFC) are also met: 

- Migration support: SQLAlchemy can support this through Alembic
- Partial model validation: SQLAlchemy supports model validators as well as
  events on attribute changes or object updates. These support sufficient
  possibilities for partial model validation.

# Detailed design

### Easy table access

Each ``Model`` can be accessed from the `ThreediDatabase` instance using 
`ThreediDatabase.modelname`. This starts a (readonly) session with autocommit=True.

### Writing: transaction management

SQLAlchemy strongly recommends working with an external `session` object. We
should not try to hide it from the end user as it is central to SQLAlchemy.

We can add a context manager that commits/rollbacks a transaction, like so:

```
@contextmanager
def session_scope():
    """Provide a transactional scope around a series of operations."""
    session = Session()
    try:
        yield session
        session.commit()
    except:
        session.rollback()
        raise
    finally:
        session.close()

```

### Columnar data access

Our data ingestion methods largely depend on numpy. A fast method to read
columns into 1D numpy arrays is required. Unfortunately, all libraries that
access SQLite only support by-records access.

The Python bindings for GDAL/OGR (Fiona) have a pending issue
https://github.com/Toblerity/Fiona/issues/469 and a fork https://github.com/brendan-ward/pyogrio 
that demonstrates columnar access. It is not likely this will result in a
working method within the next months. So we are left with by-record access.

A comparison between the current Fiona and SQLAlchemy approaches showed that
SQLAlchemy has a 5x faster read time compared to fiona/geopandas when going
through numpy structured arrays like this:

```
data = session.query(
    ST_AsBinary(models.Channel.the_geom),
    models.Channel.dist_calc_points,
    models.Channel.code,
    models.Channel.display_name,
    cast(models.Channel.calculation_type, Integer),
).all()

# transform tuples to a numpy structured array
arr = np.array(
    data, dtype=[
        ("the_geom", "O"),
        ("dist_calc_points", "f4"),
        ("code", "O"),
        ("display_name", "O"),
        ("calculation_type", "u1"),
    ]
)
```

This seems to be a promising approach. The implementation requires a mapping
from SQLAlchemy types to cast functions and numpy dtypes.

# Drawbacks

- SQLAlchemy is a relatively unknown technique for our team; Django is much
  wider known.

# Alternatives

### Use Django as ORM

Django is not only an ORM, but also a full-featured webserver. Because of this,
it is a heavyweight dependency. We do not need most features.

Also, Django does not support geopackage.

### Use geopandas as ORM

Geopandas is a heavy dependency (pandas). Also, it does not provide database
migrations and schema validations.

### Pydantic

Pydantic leverages the new python typing syntax to be able to define dataclasses.
While it is a nice approch it does not bring real advantages compared to defining
models in SQLAlchemy. In fact it adds complexity to glue the dataclasses to the
SQLAlchemy ORM. Also, we already have a full schema definition in SQLAlchemy.

# Adoption strategy

The new schematisation ORM will be part of the new modelbuilder. The old inpy
will be phased out. The migrations that are still part of the old inpy will be
put in a legacy worker to be able to keep accepting old formats.

For the rest there are no new places that require attention.

# How we teach this

A README in this repository suffices.
